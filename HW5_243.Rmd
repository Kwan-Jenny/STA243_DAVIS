---
title: "HW5"
author: "Sili Fan and Kwan Ho Lee"
date: "6/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.height = 4, fig.width = 5, fig.align = "center")

library(ggplot2)
library(stats)
library(reshape)
library(latex2exp)

```

## Problem 1



```{r}

f = function(x){
    # prob density function defined in problem specification
    1.5*dnorm((x-0.35)/0.15) - dnorm((x-0.8)/0.04)
}

## Parameterize
n = 200
is = 1:n
xis = (is-0.5)/n


noise_sd = function(j){
    # varaible noise schedule
    return(0.02+0.04*(j-1)^2)
}

## Parameterize
J = 6 # for generating noisy observations
k = 30 # number of knots
p = 3 # cubic spline
a = 0
b = 1 # bounds of inegration for lambda*int_a^b f^''(x)^2 dx penalty
knots = seq(a, b, length.out=k)

fxs = f(xis)
noise_df = data.frame(x=xis, yt=fxs)
for(j in 1:J){
    yis = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    colname = paste("y", j, sep="")
    noise_df[colname] = yis
}

# Perform cubic penalized spline regression for multiple lambda values
ex_xs = noise_df$x
ex_ys = noise_df$y3 # arbitrary noise profile for example
lam_df = data.frame(x=xis, y=fxs, yn=ex_ys)
lams = 10^seq(-10, 5, length.out=5)

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(ex_xs, 0:p, "^")
X2 = outer(ex_xs, knots, ">")*outer(ex_xs, knots, "-")^p # source [2]
X = cbind(X1, X2)
Y = ex_ys

for(lam in lams){
    H = X %*% solve(t(X) %*% X + lam*D) %*% t(X)
    fhys = H %*% Y
    colname = paste("lambda", lam, sep="=")
    lam_df[colname] = fhys
}

plt_mx = ggplot(data=lam_df) +
    geom_line(data=melt(lam_df, id=c("x", "y", "yn")),
              aes(x=x, y=value, color=variable)) +
    geom_point(aes(x=x, y=yn), alpha=0.4, shape=18, size=1.5) +
    labs(x="X", y="Y",
        title=paste(
            "Example cubic penalized regression spline with", k, "knots"
        )
    ) +
    scale_colour_discrete(
        labels=as.character(lams),
        name=TeX("Penalty $\\lambda$")
    )
plt_mx


```

### (a) Implement the cross-validation and generalized cross-validation methods for choosing the smoothing parameter λ

#### Cross validation (CV)

We interested in finding the value of $\lambda$ that minimizes the following
function for a particular sample $\{x_i,y_i\}$. We accomplish this with a simple
grid search over an appropriate range of $\lambda>0$.

$$
CV(\lambda) = \frac{1}{N}\sum_{i=1}^{N}
    (\frac{y_i-\hat{f}_\lambda(x_i)}{1-(S_\lambda)_{i,i}})^2
$$

```{r a.1, comment=NA}
cv = function(l, k=k, X=X, Y=Y){
    

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    hii = diag(H)
    r = mean(((Y-fhys)/(1-hii))^2)
    return(r)
}

cvv = Vectorize(cv, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -4, length.out=100))
cvs = cvv(X=X, Y=Y, l=lambdas, k=k)

cv_df = data.frame(x=lambdas, y=cvs)
plt_cv = ggplot(data=cv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Cross Validation Score",
         title="Cross validated values of lambda") +
    scale_x_log10() +
    # geom_hline(yintercept = min(cvs), color="coral") +
    geom_vline(xintercept = lambdas[which(cvs == min(cvs))], color="coral")
```

```{r echo=F}
lcv = lambdas[which(cvs == min(cvs))]
plt_cv
```

Note that the minimizing smoothing penalty is
$\lambda_{CV}=`r lcv`$
with a cross validation score of
$CV(\lambda_{CV})=`r min(cvs)`$.



#### Generalized cross validation (GCV)

We interested in finding the value of
$\lambda$ that minimizes the following function for a particular sample
$\{x_i,y_i\}$. We accomplish this with a simple grid search over an appropriate
range of $\lambda>0$.

$$
GCV(\lambda) = \frac{\frac{1}{N}\sum_{i=1}^{N}
    (y_i-\hat{f}_\lambda(x_i))^2}{(1-\frac{1}{N}\textrm{tr}(S_\lambda))^2}
$$

```{r a.2, comment=NA}
gcv = function(X=X, Y=Y, k=k, l){
    

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = diag(H)

    numer = mean((Y-fhys)^2)
    denom = (1-mean(tr))^2
    r = numer/denom
    return(r)
}

gcvv = Vectorize(gcv, vectorize.args = c("l"))
lambdas = 10^(seq(-9, -4, length.out=100))
gcvs = gcvv(X=X, Y=Y, l=lambdas, k=k)

gcv_df = data.frame(x=lambdas, y=gcvs)
plt_gcv = ggplot(data=gcv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Generalized Cross Validation Score",
         title="Generalized cross validated values of lambda") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(gcvs == min(gcvs))], color="coral")
```

```{r echo=F}
plt_gcv
```

Note that the minimizing smoothing penalty is
$\lambda_{GCV}=`r lambdas[which(gcvs == min(gcvs))]`$
with a generalized cross validation score of
$GCV(\lambda_{GCV})=`r min(gcvs)`$.


### (b) 

$$
AIC_C(\lambda) = \textrm{log}(||y-\hat{f}_\lambda||_2^2) +
    \frac{2(\textrm{tr}(H_\lambda)+1)}{n-\textrm{tr}(H_\lambda)-2}
$$


```{r b, comment=NA}
aicc = function(X=X, Y=Y, k=k, l){
   

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = sum(diag(H))

    r1 = log(mean((Y-fhys)^2))
    r2n = 2*(tr+1)
    r2d = n - tr - 2
    r = r1 + r2n/r2d
    return(r)
}

aiccv = Vectorize(aicc, vectorize.args = c("l"))
lambdas = 10^(seq(-8, -3, length.out=100))
aiccs = aiccv(X=X, Y=Y, l=lambdas, k=k)

aicc_df = data.frame(x=lambdas, y=aiccs)
plt_aicc = ggplot(data=aicc_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Corrected AIC Score",
         title="Corrected AIC scores for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(aiccs == min(aiccs))], color="coral")

```

```{r echo=F}
plt_aicc
```

Note that the minimizing smoothing penalty is
$\lambda_{AIC_C}=`r lambdas[which(aiccs == min(aiccs))]`$
with a score of
$AIC_C(\lambda_{AIC_C})=`r min(aiccs)`$



### (c) 

Proof:

$E(||y-\hat{f}_\lambda||^2) = ||(I-H_\lambda)f||^2 +
    \sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$
to get a sense for what this estimate will look like.

1. $E(||y-\hat{f}_\lambda||^2) = E(||(I-H_\lambda)y||^2)$ because
$\hat{f}_\lambda = H_\lambda y$ and real matricies have both left and right
distributivity.

2. $E(||(I-H_\lambda)y||^2) = E(||(I-H_\lambda)(f + e)||^2)$ because $y=f+e$.

3. $E(||(I-H_\lambda)(f + e)||^2) =
E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e)||^2)$ because $E(e)=0$ by
assumpion and the distributivity property noted in (1).

4. $E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e||^2) =
||(I-H_\lambda)f||^2 + E(||(I-H_\lambda)e)||^2$ because the lefthand quantity is
a constant.
It remains to show that $E(||(I-H_\lambda)e||^2) =
\sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$

5. $E(||(I-H_\lambda)e||^2) =
\sigma^2tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top)$ because
$Cov(e) = I\sigma^2$.

6. $tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top) = \sum_{i=1}^n (h_{ii}-1)^2$.

7. $\sum_{i=1}^n (h_{\lambda,ii} - 1)^2 =
   tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n$

To complete the proof, we get the following steps, we develop an estimator for
$risk(\lambda) = E(||f-\hat{f}_\lambda||^2)$:

1. $risk(\lambda) = E(||y-e-\hat{f}_\lambda||^2)$ by the model assumption
   $y=f+e$.
2. Expanding the $L_2$ norm in the expression above, using the linearity of
   expectation, and noting the fact that
   $E(e)=0$;
   $E(||y-e-\hat{f}_\lambda|||^2) =
    E(y^\top y - y^\top \hat{f}_\lambda + e^\top e - \hat{f}_\lambda^\top y +
    \hat{f}_\lambda^\top \hat{f}_\lambda)$
3. Consolidating terms in (2), $risk(\lambda) =
   n\sigma^2 + E(||y-\hat{f}_\lambda||^2)$ and we use the method of moments to
   create an unbiased estimator when $\sigma^2$ is known.

$y$ are the noisy observations, $\hat{f}_\lambda = H_\lambda y$ where
$H_\lambda$ depends only on $X$, and we can create an estimator for $\sigma^2$:
$\hat{\sigma^2} \approx RSS(\hat{f}_\lambda) =
\frac{1}{n} ||y-\hat{f}_\lambda||^2$. Using this estimator for $\sigma^2$,
the estimator for $E(||f-\hat{f}_\lambda||^2)$ is proportional to
$||y-\hat{f}_\lambda||^2$.

An alternative estimator for the risk, using classical pilots, is given in
section 2.2.1 of Lee (2003):
use $\hat{f}_{\lambda_p} \approx f$ where $\lambda_p$ is the $CV$ parameter.
This is the method that we employ in our algorithm.

The following algorithm exhibits this risk estimator minimization criterion for
calculating the penalty parameter $\lambda$ for a single experimental simulation
parameterization.




```{r c, comment=NA}
sighat = function(Y=Y, X=X, k=k, l){
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    r = mean((Y-fhys)^2)
    return(r)
}

er = function(X=X, Y=Y, k=k, l, lp){
   

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    Hcv = X %*% solve(t(X) %*% X + lp*D) %*% t(X)

    fcvhys = Hcv %*% Y
    fhys = H %*% Y
    sig = sighat(X=X, Y=Y, k=k, l=lp)

    I = diag(dim(H)[1])
    r = (1/n)*(sum(((H-I)%*%fcvhys)^2) + sig*sum(diag(H%*%t(H))))
    return(r)
}

erv = Vectorize(er, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -3, length.out=100))
ers = erv(X=X, Y=Y, l=lambdas, k=k, lp=lcv)

er_df = data.frame(x=lambdas, y=ers)
plt_er = ggplot(data=er_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Estimated Risk",
         title="Expected risk for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(ers == min(ers))], color="coral")

```

```{r echo=F}
plt_er
```

Note that the minimizing smoothing penalty is
$\lambda_{ER}=`r lambdas[which(ers == min(ers))]`$
with a score of
$ER(\lambda_{ER})=`r min(ers)`$.


### (d) Conduct a simulation study to compare the above four methods for choosing λ. Use the experimental setting from the paper downloadable from Canvas. 

In this homework, we simulate six different methods for selecting a penalty
parameter $\lambda$ under six different noise, design density, spatial variation,
and variance heterogeneity states.
The four methods for fitting the penalty parameter
$\lambda$ for a penalized cubic regression spline are minimization of the
following scores: cross validation, generalized cross validation, corrected
Akaike information criterion, and expected risk.

In this part, we illustrate the concepts required for the simulation in the
context of a simple example: $y=f(x)+\epsilon$.

To fit a cubic penalized regression spline $\hat{f}_\lambda(x)$ which
approximates $f(x)$, we minimize the following functional within the class
$\mathbf{F}$ of cubic regression splines:

$$
\hat{f}_\lambda(x) =
\textrm{argmin}_{f\in\mathbf{F}} \sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda\sum_{j=1}^{k} \beta_{p_j}^2 =
||y-f(x)||_2^2 + \lambda\beta^\top D\beta
$$

where $D$ is the diagonal matrix with $p+1$ $0$s followed by $K$ $1$s.

#### Performance under different levels of noise
We evaluate the performance of $\lambda$ estimators under the model
$y_{ij} = f(x_i) + \sigma_j\epsilon_i$ where $\sigma_j= 0.02 + 0.04(j − 1)^2$
  and $\epsilon_i \sim\textrm{iid}N(0,1)$.

```{r d.1, comment=NA}

## Parameterize
J = 6 # number of simulation parameterizations
M = 20 # number of simulations
L = 150 # number of lambdas to grid search
p = 3 # dimension of spline
k = 30 # number of knots
n = 200 # number of observations
a = 0
b = 1
knots = seq(a, b, length.out=k)
xs = ((1:n)-0.5)/n
fxs = f(xs)
lambdas = 10^seq(-10,0, length.out=L)


# Data storage objects

noise_raw_df = data.frame(x=xs, yt=fxs) # observations for plotting
noise_lam_df = data.frame( # best lambda for each simulation 1:M
  cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
  J=rep(NA, J*M))
noise_sco_df = data.frame( # score of the corresponding best lambda
  cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
  J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p 
X = cbind(X1, X2)

for(j in 1:J){
  
  for(i in 1:M){
    
    Y = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    
    # calculate lambda using each method
    cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
    gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
    aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)
    
    bcv = min(cvs) 
    cv_lam = lambdas[which(cvs == bcv)]
    
    ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) 
    
    bgcv = min(gcvs)
    gcv_lam = lambdas[which(gcvs == bgcv)]
    baicc = min(aiccs)
    aicc_lam = lambdas[which(aiccs == baicc)]
    ber = min(ers)
    er_lam = lambdas[which(ers == ber)]
    
    ix = (j-1)*M + i
    noise_lam_df$J[ix] = j
    noise_lam_df$cv[ix] = cv_lam
    noise_lam_df$gcv[ix] = gcv_lam
    noise_lam_df$aic[ix] = aicc_lam
    noise_lam_df$er[ix] = er_lam
    
    noise_sco_df$cv[ix] = bcv
    noise_sco_df$gcv[ix] = bgcv
    noise_sco_df$aic[ix] = baicc
    noise_sco_df$er[ix] = ber
    
  }
  
  
  colname = paste("J=", j, sep="")
  noise_raw_df[colname] = Y
}

## Plot results

plt_noise_obv = ggplot(data=melt(noise_raw_df, id=c("x", "yt"))) +
  geom_line(data=noise_raw_df, aes(x=x, y=yt), color="steelblue") +
  geom_point(aes(x=x, y=value, color=variable),
             alpha=0.7, size=1.5, shape=18) +
  facet_wrap(~variable) +
  scale_colour_discrete(
    labels=as.character(1:J),
    name="Noise regime (J)"
  ) +
  labs(x="X", y="Y", title="Observations with different noise")

noise_lam_df_m = melt(noise_lam_df, id=c("J"))
noise_lam_df_m$J = as.factor(noise_lam_df_m$J)

ylims = boxplot.stats(noise_lam_df_m$value)$stats[c(1, 5)]

plt_noise_lams = ggplot(data=noise_lam_df_m) +
  geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
  coord_cartesian(ylim = ylims*1.05) +
  facet_wrap(~variable) +
  scale_y_log10() +
  scale_colour_discrete(
    labels=as.character(1:J),
    name="Noise regime (J)"
  ) +
  labs(x="Noise regime", y=TeX("Optimal $\\lambda$"),
       title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_noise_obv
```

The above plot illustrates the effects of noise regimes $j\in\{1,\dots,J=6\}$.

```{r echo=F}
plt_noise_lams
```

As the level of noise increases, the optimal penalty parameter increases and its
stability decreases. The increase in $\lambda$ means that we encourage the spline
to be smoother - to fit less tightly to the noisy data - as the level of noise
increases.

#### Performance under different design density

$$
  y_{ij} = f(X_{ij}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.1, X_{ij} = F^{-1}(X_i),
F(X) = \textrm{Beta}(\frac{j+4}{5}, \frac{11-j}{5}), X_i\sim \textrm{Unif}(0,1)
$$

```{r d.2, comment=NA}

## Parameterize
lambdas = 10^seq(-9,-4, length.out=L)

# Data storage objects
xts = ((1:n)-0.05)/n
yts = f(xts)
dens_raw_df = data.frame(x=xts, y=yts, j=rep(0, n)) # observations for plotting
dens_lam_df = data.frame( # best lambda for each simulation 1:M
  cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
  J=rep(NA, J*M))
dens_sco_df = data.frame( # score of the corresponding best lambda
  cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
  J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))

for(j in 1:J){
  
  for(i in 1:M){
    
    
    xs = qbeta(runif(n), (j+4)/5, (11-j)/5)
    fxs = f(xs)
    X1 = outer(xs, 0:p, "^")
    X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p 
    X = cbind(X1, X2)
    
    
    Y = fxs + rnorm(n, mean=0, sd=0.1)
    
    # calculate lambda using each method
    cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
    gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
    aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)
    
    bcv = min(cvs) 
    cv_lam = lambdas[which(cvs == bcv)]
    
    ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) 
    
    bgcv = min(gcvs)
    gcv_lam = lambdas[which(gcvs == bgcv)]
    baicc = min(aiccs)
    aicc_lam = lambdas[which(aiccs == baicc)]
    ber = min(ers)
    er_lam = lambdas[which(ers == ber)]
    
    ix = (j-1)*M + i
    dens_lam_df$J[ix] = j
    dens_lam_df$cv[ix] = cv_lam
    dens_lam_df$gcv[ix] = gcv_lam
    dens_lam_df$aic[ix] = aicc_lam
    dens_lam_df$er[ix] = er_lam
    
    dens_sco_df$cv[ix] = bcv
    dens_sco_df$gcv[ix] = bgcv
    dens_sco_df$aic[ix] = baicc
    dens_sco_df$er[ix] = ber
    
  }
  
  
  tdf = data.frame(x=xs, y=Y, j=rep(j, n)) 
  dens_raw_df = rbind(dens_raw_df, tdf)
}

dens_raw_df$j = as.factor(dens_raw_df$j)
drdf0 = subset(dens_raw_df, j == 0, select=c("x","y"))
plt_dens_obv = ggplot(data=drdf0) +
  geom_line(aes(x=x,y=y), color="steelblue") +
  geom_point(data=subset(dens_raw_df, j != 0), aes(x=x, y=y, color=j),
             shape=18, size=1.5) +
  facet_wrap(~j) +
  scale_colour_discrete(
    labels=as.character(1:J),
    name="Support variance regime (J)"
  ) +
  labs(x="X", y="Y", title="Observations with variable support density")

dens_lam_df_m = melt(dens_lam_df, id=c("J"))
dens_lam_df_m$J = as.factor(dens_lam_df_m$J)
ylims = boxplot.stats(dens_lam_df_m$value)$stats[c(1, 5)]
plt_dens_lams = ggplot(data=dens_lam_df_m) +
  geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
  scale_y_log10() +
  coord_cartesian(ylim = ylims*1.05) +
  facet_wrap(~variable) +
  scale_colour_discrete(
    labels=as.character(1:J),
    name="Support variance regime (J)"
  ) +
  labs(x="Support variance regime", y=TeX("Optimal $\\lambda$"),
       title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_dens_obv
```

As illustrated above, the small values of $j$ group points to the left-hand side
of the support, while the large values of $j$ group points to the right-hand
side.

```{r echo=F}
plt_dens_lams
```

The stability of the penalty parameter estimations is much lower when the points
are not homogeneously dense throughout the support. This is especially true
considering that we fitting a spline with `r k` equally spaced knots. A variable
knot selection protocol might mitigate some of the instability of the penalty
parameter estimates observed for $j\in\{1,2,5,6\}$.


#### Performance under different frequency of target function

$$
y_{ij} = f_j(x_{i}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.2,
f_j(x) = \sqrt{x*(1-x)}\textrm{sin}(\frac{2\pi(1+2^{(9-4j)/5})}{
x+2^{(9-4j)/5}}), \epsilon_i\sim \textrm{iid}N(0,1)
$$

```{r d.3, comment=NA}

## Parameterize
xs = ((1:n)-0.5)/n
lambdas = 10^seq(-11,-1, length.out=L)


ff = function(x, j){
    r = sqrt(x*(1-x))*sin((2*pi*(1+2^((9-4*j)/5)))/(x+2^((9-4*j)/5)))
    return(r)
}

# Data storage objects

sp_raw_df = data.frame(x=NA, yt=NA, ys=NA, j=NA) 
sp_lam_df = data.frame( 
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p 
X = cbind(X1, X2)

for(j in 1:J){

    fxs = ff(xs, j)

    for(i in 1:M){

        
        Y = fxs + rnorm(n, mean=0, sd=0.2)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) 
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) 

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        sp_lam_df$J[ix] = j
        sp_lam_df$cv[ix] = cv_lam
        sp_lam_df$gcv[ix] = gcv_lam
        sp_lam_df$aic[ix] = aicc_lam
        sp_lam_df$er[ix] = er_lam

    }

    
    tdf = data.frame(x=xs, yt=fxs, ys=Y, j=rep(j, n))
    sp_raw_df = rbind(sp_raw_df, tdf)
}

sp_raw_df = subset(sp_raw_df, ! is.na(j)) 
sp_raw_df$j = as.factor(sp_raw_df$j)
plt_sp_obv = ggplot(data=sp_raw_df) +
    geom_line(aes(x=x,y=yt), color="steelblue") +
    geom_point(aes(x=x, y=ys, color=j), shape=18, size=1.5) +
    facet_wrap(~j) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Function frequency regime (J)"
    ) +
    labs(x="X", y="Y",
        title="Observations generated by different target function frequencies")

sp_lam_df_m = melt(sp_lam_df, id=c("J"))
sp_lam_df_m$J = as.factor(sp_lam_df_m$J)
plt_sp_lams = ggplot(data=sp_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J)) +
    scale_y_log10() +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Function frequency regime (J)"
    ) +
    labs(x="Target function frequencies", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_sp_obv
```
  
As illustrated above, increasing $j$ increases the frequency of the spatial
distribution function $f(x)$.

```{r echo=F}
plt_sp_lams
```
  
Increased spatial frequency stabilizes the estimates of the
penalty parameter to a point. Furthermore, as the function becomes more erratic,
the value of the penalty parameter $\lambda$ decreases, forcing the spline to
hug the data more closely.



#### Performance under heterogeneous variances

$$
y_{ij} = f(x_{i}) + \sqrt{v_j(x_i)}\epsilon_i \quad \textrm{where} \quad
v_j(x) = (0.15(1 + 0.4(2j-7)(x-0.5)))^2
$$
and $f(x)$ and $\epsilon_i$ are defined as in the above.

```{r d.4, comment=NA}

## Parameterize

xs = ((1:n)-0.5)/n
lambdas = 10^seq(-11,-1, length.out=L)


fv = function(x, j){
    r = (0.15*(1+0.4*(2*j-7)*(x-0.5)))^2
    return(sqrt(r))
}


# Data storage objects

v_raw_df = data.frame(x=NA, yt=NA, ys=NA, j=NA) 
v_lam_df = data.frame( 
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p 
X = cbind(X1, X2)
fxs = f(xs)

for(j in 1:J){

    for(i in 1:M){

        
        Y = fxs + fv(xs, j)*rnorm(n, mean=0, sd=1)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) 
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) 

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        sp_lam_df$J[ix] = j
        sp_lam_df$cv[ix] = cv_lam
        sp_lam_df$gcv[ix] = gcv_lam
        sp_lam_df$aic[ix] = aicc_lam
        sp_lam_df$er[ix] = er_lam

    }

    
    tdf = data.frame(x=xs, yt=fxs, ys=Y, j=rep(j, n)) 
    v_raw_df = rbind(v_raw_df, tdf)
}

v_raw_df = subset(v_raw_df, ! is.na(j)) 
v_raw_df$j = as.factor(v_raw_df$j)
plt_v_obv = ggplot(data=v_raw_df) +
    geom_line(aes(x=x,y=yt), color="steelblue") +
    geom_point(aes(x=x, y=ys, color=j), shape=18, size=1.5) +
    facet_wrap(~j) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Variance regime (J)"
    ) +
    labs(x="X", y="Y",
        title="Observations generated by different variance structures")

v_lam_df_m = melt(sp_lam_df, id=c("J"))
v_lam_df_m$J = as.factor(v_lam_df_m$J)
plt_v_lams = ggplot(data=v_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J)) +
    scale_y_log10() +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Variance regime (J)"
    ) +
    labs(x="Variance structures", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_v_obv
```
  
As illustrated above, this method for varying the variance spatially incurs a
high variance for the left-hand side when $j$ is small and a high variance for
the right-hand side when $j$ is large.

```{r echo=F}
plt_v_lams
```
  
Spatial variance modulation destabilizes smoothing parameter estimates. Having
`r k` equi-spaced knots is a serious issue here and a more dynamic collocation
procedure would ameliorate some of this instability in the $\lambda$ estimates.



\newpage

## Appendix

The following code is used in Problem 1 part (a)
```{r ref.label='a.1',eval=FALSE,echo=TRUE}
```
```{r ref.label='a.2',eval=FALSE,echo=TRUE}
```

The following code is used in Problem 1 part (b)
```{r ref.label='b',eval=FALSE,echo=TRUE}
```

The following code is used in Problem 1 part (c)
```{r ref.label='c',eval=FALSE,echo=TRUE}
```

The following code is used in Problem 1 part (d)
```{r ref.label='d.1',eval=FALSE,echo=TRUE}
```
```{r ref.label='d.2',eval=FALSE,echo=TRUE}
```
```{r ref.label='d.3',eval=FALSE,echo=TRUE}
```
```{r ref.label='d.4',eval=FALSE,echo=TRUE}
```
